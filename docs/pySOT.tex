\documentclass[]{article}

% definitions
\def\D{\partial}

\def\solidline{{$\textcolor{blue}{\overline{\hskip 0.5cm}}\ $}}
\def\dashedline{{$\textcolor{red}{\overline{\hskip 0.1cm}\ \overline{\hskip 0.1cm}\ \overline{\hskip 0.1cm}\ }$}}
\def\dashdottedline{$\overline{\hskip 0.1cm}\ \overline{\hskip 0.03cm}\ \overline{\hskip 0.1cm}\ $}
\def\dottedline{$\overline{\hskip 0.04cm}\ \overline{\hskip 0.04cm}\ \overline{\hskip 0.04cm}\ $}

\usepackage{amsmath,amssymb,amsthm}
\newtheorem{thm}{Theorem}
\usepackage{lettrine}
\usepackage{amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{bigints}
\usepackage{gensymb}
\usepackage{float}
\usepackage{color}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry}
\usepackage[font=it]{caption}
\usepackage{paralist}
\usepackage{multicol}
\usepackage{lettrine}
\usepackage[nodayofweek]{datetime}
\usepackage{centernot}
\usepackage{ mathrsfs }
\usepackage{textcomp}
\usepackage{color}
%\usepackage{listings}
\usepackage{setspace}

\usepackage{color}
\usepackage[procnames]{listings}
\usepackage{setspace}
\usepackage{palatino}
\renewcommand{\lstlistlistingname}{Code Listings}
\renewcommand{\lstlistingname}{Code Listing}
\definecolor{gray}{gray}{0.5}
\definecolor{green}{rgb}{0,0.5,0}
\definecolor{lightgreen}{rgb}{0,0.7,0}
\definecolor{purple}{rgb}{0.5,0,0.5}
\definecolor{darkred}{rgb}{0.5,0,0}
\lstnewenvironment{python}[1][]{
\lstset{
language=python,
basicstyle=\ttfamily\small\setstretch{1},
stringstyle=\color{green},
showstringspaces=false,
alsoletter={1234567890},
otherkeywords={\ , \}, \{},
keywordstyle=\color{blue},
emph={access,and,as,break,class,continue,def,del,elif,else,%
except,exec,finally,for,from,global,if,import,in,is,%
lambda,not,or,pass,print,raise,return,try,while,assert},
emphstyle=\color{orange}\bfseries,
emph={[2]self},
emphstyle=[2]\color{gray},
emph={[4]ArithmeticError,AssertionError,AttributeError,BaseException,%
DeprecationWarning,EOFError,Ellipsis,EnvironmentError,Exception,%
False,FloatingPointError,FutureWarning,GeneratorExit,IOError,%
ImportError,ImportWarning,IndentationError,IndexError,KeyError,%
KeyboardInterrupt,LookupError,MemoryError,NameError,None,%
NotImplemented,NotImplementedError,OSError,OverflowError,%
PendingDeprecationWarning,ReferenceError,RuntimeError,RuntimeWarning,%
StandardError,StopIteration,SyntaxError,SyntaxWarning,SystemError,%
SystemExit,TabError,True,TypeError,UnboundLocalError,UnicodeDecodeError,%
UnicodeEncodeError,UnicodeError,UnicodeTranslateError,UnicodeWarning,%
UserWarning,ValueError,Warning,ZeroDivisionError,abs,all,any,apply,%
basestring,bool,buffer,callable,chr,classmethod,cmp,coerce,compile,%
complex,copyright,credits,delattr,dict,dir,divmod,enumerate,eval,%
execfile,exit,file,filter,float,frozenset,getattr,globals,hasattr,%
hash,help,hex,id,input,int,intern,isinstance,issubclass,iter,len,%
license,list,locals,long,map,max,min,object,oct,open,ord,pow,property,%
quit,range,raw_input,reduce,reload,repr,reversed,round,set,setattr,%
slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,%
vars,xrange,zip},
emphstyle=[4]\color{purple}\bfseries,
upquote=true,
morecomment=[s][\color{lightgreen}]{"""}{"""},
commentstyle=\color{red}\slshape,
literate={>>>}{\textbf{\textcolor{darkred}{>{>}>}}}3%
         {...}{{\textcolor{gray}{...}}}3,
procnamekeys={def,class},
procnamestyle=\color{blue}\textbf,
framexleftmargin=1mm, framextopmargin=1mm, frame=shadowbox,
rulesepcolor=\color{blue},#1
}}{}




%%%%% Titling
\usepackage[compact]{titlesec}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\newenvironment{corollary}[1]{\par\noindent\underline{Corollary:}\space#1}{}
\newenvironment{lemma}[1]{\par\noindent\underline{Lemma:}\space#1}{}
\newenvironment{theorem}[1]{\par\noindent\underline{Theorem:}\space#1}{}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{proposition}[1]{\par\noindent\underline{Proposition:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill\ensuremath{\square}}

\include{pythonlisting}

% ------
% Header/footer
\usepackage{fancyhdr}
	\pagestyle{fancy}
	\fancyhead{}
	\fancyfoot{}
	\fancyhead[C]{Surrogate Optimization Toolbox (pySOT) - 0.1.0 $\bullet$ Tutorial $\bullet$ David Eriksson $\bullet$ \today}
	\fancyfoot[RO,LE]{\thepage}

%%% ---- MATLAB CODE ---- %%%

\definecolor{Blue}{rgb}{0.1,0.1,0.3}
\hypersetup{colorlinks=true,linkcolor=Blue,citecolor=Blue,urlcolor=Blue}

\definecolor{listinggray}{gray}{0.9} 
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{dorange}{rgb}{1.000000,0.549020,0.000000}
\definecolor{lblue}{rgb}{0.529412,0.807843,0.980392}
\lstset{escapeinside={<@}{@>}}


\definecolor{orange}{rgb}{1,0.5,0}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\X}{\mathcal{X}}
\DeclareMathOperator{\Y}{\mathcal{Y}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Fx}{\mathcal{F}_{\X}}
\DeclareMathOperator{\Fy}{\mathcal{F}_{\Y}}
\DeclareMathOperator{\Fz}{\mathcal{F}_{\Z}}
\DeclareMathOperator{\Nc}{\mathcal{N}}
\DeclareMathOperator{\Rc}{\mathcal{R}}
\DeclareMathOperator{\B}{\mathcal{B}}
\DeclareMathOperator{\C}{\mathbb{C}}
\DeclareMathOperator{\Rb}{\mathbb{R}}
\DeclareMathOperator{\Sb}{\mathcal{S}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\limn}{\lim\limits_{n \to \infty}}
\DeclareMathOperator{\limm}{\lim\limits_{m \to \infty}}
\DeclareMathOperator{\R}{Re}
\DeclareMathOperator{\I}{Im}
\DeclareMathOperator{\spann}{span}
\newcommand*{\QED}{\hfill\ensuremath{\square}}%

%%%% Title
\title{\vspace{-15mm}%
	\fontsize{18pt}{10pt}\selectfont
	\textbf{Surrogate Optimization Toolbox (pySOT) - 0.1.0 \\ Tutorial}
	}	
\author{%
	\Large\textsc{David Eriksson} \\[2mm]
		\normalsize	Cornell University \\
	\normalsize Center for Applied Mathematics \\
	\normalsize	dme65@cornell.edu \\ 
	}
\date{\today}

\begin{document}
\fontsize{12}{14}\rm

\maketitle
\thispagestyle{fancy}
\tableofcontents

\section{Introduction}
This is a tutorial (user guide) for the Surrogate Optimization Toolbox (pySOT) for global deterministic optimization problems. The main purpose of the toolbox is for optimization of computationally expensive black-box objective functions with continuous and/or integer variables. We support inequality constraints of any form through a penalty method approach, but cannot yet efficiently handle equality constraints. All variables are assumed to have bound constraints in some form where none of the bounds are infinity. The tighter the bounds, the more efficient are the algorithms since it reduces the search region and increases the quality of the constructed surrogate. The longer the objective functions are to evaluate, the more efficient are these algorithms. For this reason, this toolbox may not be very efficient for problems with computationally cheap function evaluations. Surrogate models are intended to be used when function evaluations take from several minutes to several hours or more. The toolbox is based on the following published papers that should be cited when the toolbox is used for own research purposes:
\begin{enumerate}
\item J. Muller and R. Piche, 2011. "Mixture Surrogate Models Based on Dempster-Shafer Theory for Global Optimization Problems", Journal of Global Optimization, vol. 51, pp. 79-104
\item J. Muller, C.A. Shoemaker, and R. Piche, 2012. "SO-MI: A Surrogate Model Algorithm for Computationally Expensive Nonlinear Mixed-Integer Black-Box Global Optimization Problems", Computers \& Operations Research, \\ http://dx.doi.org/10.1016/j.cor.2012.08.022
\item R.G. Regis and C.A. Shoemaker, 2007. "A Stochastic Radial Basis Function Method for the Global Optimization of Expensive Functions", INFORMS Journal on Computing, vol. 19, pp. 497-509
\item R.G. Regis and C.A. Shoemaker, 2009. "Parallel Stochastic Global Optimization Using Radial Basis Functions", INFORMS Journal on Computing, vol. 21, pp. 411-426
\end{enumerate}
For easier understanding of the algorithms in this toolbox, it is recommended and helpful to read these papers. If you have any questions, or you encounter any bugs, please feel free to either submit a bug report on Github (recommended) or to contact me at the email address: dme65@cornell.edu.

\section{Licensing} Please refer to LICENSE.txt

\section{Surrogate Model Algorithms}
Surrogates models (or response surfaces) are used to approximate an underlying function that has been evaluated for a set of points. During the optimization phase information from the surrogate model is used in order to guide the search for improved solutions, which has the advantage of not needing as many function evaluations to find a good solution. Most surrogate model algorithms consist of the same steps as shown in the algorithm below.
\begin{enumerate}
\item Generate an initial experimental design.
\item Carry out the costly function evaluations at the points generated in Step 1.
\item Fit a response surface to the data generated in Steps 1 and 2.
\item Use the response surface to predict the objective function values at new points in the variable domain in order to decide the next point(s) to be evaluated.
\item Do the expensive function evaluation at the point(s) selected in Step 4.
\item Use the new data to update the surrogate model.
\item Iterate through Steps 4 to 6 until the stopping criterion has been met.
\end{enumerate}

\noindent Surrogate model algorithms in the literature differ mainly with respect to

\begin{itemize}
\item The generation of the initial experimental design;
\item The chosen surrogate model;
\item The strategy for selecting the sample point(s) in each iteration.
\end{itemize}

\noindent Typically used stopping criteria are a maximum number of allowed function evaluations (used in this toolbox), a maximum allowed CPU time, or a maximum number of failed iterative improvement trials.

\section{Installation}
Before starting you will need Python 2.7 and pypi (pip).  There are currently two ways to install the toolbox:
\begin{enumerate}
\item The easiest way to install the toolbox is through pypi in which case the following command should suffice (you may need sudo for UNIX):
\begin{python}
pip install pySOT
\end{python}
\item 
\begin{enumerate}
\item Clone the repository: 
\begin{python}
git clone https://github.com/dme65/pySOT
\end{python} 
or alternatively download the repository directly:
\begin{enumerate}
\item Go to https://github.com/dme65/pySOT
\item Download the repository, extract the zip folder and change the name to pySOT
\end{enumerate}
\item Navigate to the repository using:
\begin{python}
cd pySOT
\end{python} 
\item Install dependencies:
\begin{python}
pip install -r ./requirements.txt
\end{python} 
\item Install pySOT (you may need to use sudo for UNIX):
\begin{python}
python setup.py install
\end{python} 
\item Run a 30 dimensional test problem of the Ackley objective function with 4 threads and 1000 evaluations: 
\begin{python}
python ./pySOT/test/test.py
\end{python}
\end{enumerate}
\end{enumerate}
\ \newline \textbf{Optional: } If you want to use MARS you need to install the py-earth toolbox (http://github.com/jcrudy/py-earth)  

\section{Sphinx documentation}
The necessary files to build the Sphinx documentation are provided in the docs subdirectory. We use the napoleon extension so you need to make sure you have this package. This can be done through pip
\begin{python}
pip install sphinxcontrib-napoleon
\end{python}
To build the documentation run the command:
\begin{python}
make html
\end{python}


\section{Options}
These are the the components and the supported options:
\subsection{Experimental design} The experimental design generates the initial points to be evaluated. A well-chosen experimental design is critical in order to fit a Surrogate model that captures the behavior of the underlying objective function. The following experimental designs are supported:
\begin{itemize}
\item \textbf{LatinHypercube}. Arguments:
\begin{itemize}
\item \textbf{dim:} Number of dimensions
\item \textbf{npts:} Number of points to generate ($2 \text{dim}+1$ is recommended)
\end{itemize} 
\ \newline Example: 
\begin{python}
from pySOT import LatinHypercube
LatinHypercube(dim=3, npts=10)
\end{python}
creates a Latin hypercube design with 10 points in 3 dimensions
\item \textbf{SymmetricLatinHypercube} Arguments:
\begin{itemize}
\item \textbf{dim:} Number of dimensions
\item \textbf{npts:} Number of points to generate ($2 \text{dim}+1$ is recommended)
\end{itemize}
\ \newline Example: 
\begin{python}
from pySOT import SymmetricLatinHypercube
SymmetricLatinHypercube(dim=3, npts=10)
\end{python}
creates a symmetric Latin hypercube design with 10 points in 3 dimensions

\end{itemize}

\subsection{Surrogate model} The surrogate model approximates the underlying objective function given all of the points that have been evaluated. The following surrogate models are supported:
\begin{itemize}
\item \textbf{RBFInterpolant}. A radial basis function interpolant. Arguments:
\begin{itemize}
\item \textbf{phi}: Kernel function. The options are 
\begin{itemize}
\item \textbf{phi\_linear:} Linear RBF
\item \textbf{phi\_cubic:} Cubic RBF
\item \textbf{phi\_plate:} Thin-Plate RBF
\end{itemize}
\item \textbf{P:} Tail functions. The options are
\begin{itemize}
\item \textbf{const\_tail:} Constant tail
\item \textbf{linear\_tail:} Linear tail
\end{itemize}
\item \textbf{dphi:} Derivative of kernel function. The options are:
\begin{itemize}
\item \textbf{dphi\_linear:} Derivative of Linear RBF
\item \textbf{dphi\_cubic:} Derivative of Cubic RBF
\item \textbf{dphi\_plate:} Derivative of Thin-Plate RBF
\end{itemize}
\item \textbf{dP:} Gradient of tail functions. The options are:
\begin{itemize}
\item \textbf{dconst\_tail:} Derivative of Constant tail
\item \textbf{dlinear\_tail:} Derivative of Linear tail
\end{itemize}
\item \textbf{eta:} Regularization parameter. Default is 1e-8
\item \textbf{maxp:} Initial maximum number of points (can grow). Default is 100.
\end{itemize}
\ \newline Example:
\begin{python}
from pySOT import RBFInterpolant, phi_cubic, linear_tail, \
	          dphi_cubic, dlinear_tail
RBFInterpolant(phi=phi_cubic, P=linear_tail, dphi=dphi_cubic, 
               dP=dlinear_tail, eta=1e-8, maxp=500)
\end{python}
creates a cubic RBF with a linear tail with a capacity for 500 points.
\item \textbf{KrigingInterpolant:} A Kriging interpolant. Arguments:
\begin{itemize}
\item \textbf{initp:} Number of initial points for setting up Kriging model. Use the same number of points that you generate from the experimental design. The Kriging model is not used before all points in the experimental design have been evaluated.
\item \textbf{maxp:} Maximum number of points (can grow). Default is 100
\end{itemize}
\ \newline Example:
\begin{python}
from pySOT import KrigingInterpolant
KrigingInterpolant(initp=20, maxp=500)
\end{python}
generates a Kriging interpolant that needs 20 initial points and a capacity of 500 points.
\item \textbf{MARSInterpolant:} Generate a Multivariate Adaptive Regression Splines (MARS) model. Arguments:
\begin{itemize}
\item \textbf{maxp:} Maximum number of points (can grow). Default is 100
\end{itemize}
\ \newline Example: 
\begin{python}
from pySOT import MARSInterpolant
MARSInterpolant(maxp=500)
\end{python}
generates a MARS interpolant with a capacity of 500 points.
\end{itemize}
\textbf{Note:} The user is responsible for resetting the response surface after each experiment and this is done by calling the reset() method.

\subsection{Capped RBF model} Functions with exponential behaviors can cause the fitted surface to oscillate wildly. In the case of the RBFInterpolant we therefore provide a capping strategy that replaces every value above the median by the median value. This adapter takes an existing response surface and replaces it with a modified version in which any function values above the median are replaced by the median value. \\
\ \newline Example: 
\begin{python}
from pySOT import RSCapped, RBFInterpolant, phi_cubic, linear_tail, \
	          dphi_cubic, dlinear_tail
RSCapped(RBFInterpolant(phi=phi_cubic, P=linear_tail, dphi=dphi_cubic, 
                        dP=dlinear_tail, eta=1e-8, maxp=500))
\end{python}
creates a cubic RBF with a linear tail with a capacity for 500 points with capping.

\subsection{Constraint Method} In order to handle general inequality constraints we have implemented a simple penalty method that starts with a penalty $\mu$ set by the user and then solves the box-constrained optimization problem 
\begin{align*}
\underset{x}{\operatorname{minimize}} \qquad &f(x)+\mu \sum_{i=1}^M \max(0,g_i(x))^2 \\
\operatorname{subject\;to:} \qquad &-\infty<\ell_i \leq x_i \leq u_i<\infty, \quad i = 1,\ldots,n \\
\end{align*}
where $x \in \Rb^n$ and there are $M$ inequality constraints of the form $g_i(x) \leq 0,$ for $i=1,\ldots,M$. If the best solution found is infeasible the penalty is multiplied by 10 and the algorithm restarts. This is carried out until the best solution found is feasible.
\ \newline \ \newline Example: 
\begin{python}
from pySOT import PenaltyMethod
PenaltyMethod(penalty=1.0)
\end{python}
\ \newline \noindent \textbf{Note:} The user is responsible for resetting the constraint handler after each experiment and this is done by calling the reset() method. The constraint handler is not reset to make the penalty that gave a feasible solution accessible for the user, so that this value can guide the penalty for the next experiment.

\subsection{Objective function} The objective function is its own object and must have certain attributes and methods in order to work with the framework. We start by giving an example of a mixed-integer optimization problem with constraints. The following attributes must always be specified in the objective function class:
\begin{itemize}
\item \textbf{xlow:} Lower bounds for the variables.
\item \textbf{xup:} Upper bounds for the variables.
\item \textbf{dim:} Number of dimensions
\item \textbf{integer:} Specifies the integer variables. If no variables have integer constraints, set to [\,]
\item \textbf{continuous:} Specifies the continuous variables. If no variables are continuous, set to [\,]
\item \textbf{constraints:} Set to True if there are inequality constraints in addition to \\ the bound constraints. Set to False otherwise
\end{itemize}
\ \newline The following methods must also exist.
\begin{itemize}
\item \textbf{objfunction:} Takes one input in the form of an numpy.ndarray, which corresponds to one point in dim dimensions. Returns the value of the objective function
\item \textbf{eval\_ineq\_constraints:}  Only necessary if the attribute constraints is true. Takes one input in the form of an numpy.ndarray, which corresponds to one point in dim dimensions. Returns a numpy.matrixlib.defmatrix.matrix of size $1 \times M$ where $M$ is the number of inequality constraints that are not bound constraints. 
\end{itemize}
\ \newline \noindent What follows is an example of an objective function in 5 dimensions with 3 integer and 2 continuous variables. There are also 3 inequality constraints that are not bound constraints which means that we need to implement the eval\_ineq\_constraints method.
\begin{python}
import numpy as np

class LinearMI:
    def __init__(self):
        self.xlow = np.zeros(5)
        self.xup = np.array([10, 10, 10, 1, 1])
        self.dim = 5
        self.min = -1
        self.integer = np.arange(0, 3)
        self.continuous = np.arange(3, 5)
        self.constraints = True

    def eval_ineq_constraints(self, x):
        vec = np.zeros((3,))
        vec[0] = x[0] + x[2] - 1.6
        vec[1] = 1.333 * x[1] + x[3] - 3
        vec[2] = - x[2] - x[3] + x[4]
        return vec

    def objfunction(self, x):
        if len(x) != self.dim:
            raise ValueError('Dimension mismatch')
        return - x[0] + 3 * x[1] + 1.5 * x[2] + 2 * x[3] - 0.5 * x[4]
\end{python}
\textbf{Note:} The method validate in test\_problems.py is helpful in order to test that the objective function is compatible with the framework. \newline


\subsection{Generation of next point to evaluate} We provide several different methods for selecting the next point to evaluate. All methods in this version are based in generating candidate points by perturbing the best solution found so far or in some cases just choose a random point. We also provide the option of using many different strategies in the same experiment and how to cycle between the different strategies. We start by listing all the different options and describe shortly how they work.
\begin{itemize}
\item \textbf{CandidateSRBF:} Generate perturbations around the best solution found so far
\item \textbf{CandidateSRBF\_INT:} Uses CandidateSRBF but only perturbs the integer variables
\item \textbf{CandidateSRBF\_CONT:} Uses CandidateSRBF but only perturbs the continuous variables
\item \textbf{DyCORS} Uses a DDS strategy which perturbs each coordinate with some iteration dependent probability. This probability is a monotonically decreasing function with the number of iteration.
\item \textbf{CandidateDyCORS\_CONT:} Uses CandidateDyCORS but only perturbs the continuous variables
\item \textbf{CandidateDyCORS\_INT:} Uses CandidateDyCORS but only perturbs the integer variables
\item \textbf{CandidateUniform:} Chooses a new point uniformly from the box-constrained domain
\item \textbf{CandidateUniform\_CONT:} Given the best solution found so far the continuous variables are chosen uniformly from the box-constrained domain
\item \textbf{CandidateUniform\_INT:} Given the best solution found so far the integer variables are chosen uniformly from the box-constrained domain
\end{itemize}
The CandidateDyCORS algorithm is the bread-and-butter algorithm for any problems with more than 5 dimensions whilst CandidateSRBF is recommended for problems with only a few dimensions. It is sometimes efficient in mixed-integer problems to perturb the integer and continuous variables separately and we therefore provide such method for each of these algorithms. Finally, uniformly choosing a new point has the advantage of creating diversity to avoid getting stuck in a local minima. Each method needs an objective function object as described in the previous section (the input name is data) and how many perturbations should be generated around the best solution found so far (the input name is numcand). Around 100 points per dimension, but no more than 5000, is recommended. Next is an example on how to generate a multi-start strategy that uses CandidateDyCORS, CandidateDyCORS\_CONT, CandidateDyCORS\_INT, and CandidateUniform and that cycles evenly between the methods i.e., the first point is generated using CandidateDyCORS, the second using CandidateDyCORS\_CONT and so on.
\begin{python}
from pySOT import LinearMI, MultiSearchStrategy, CandidateDyCORS, \
			  CandidateDyCORS_CONT, CandidateDyCORS_INT, \
			  CandidateUniform

data = LinearMI()  # Optimization problem
search_strategies = [CandidateDyCORS(data=data, numcand=100*data.dim),
                     CandidateDyCORS_CONT(data=data, numcand=100*data.dim),
                     CandidateDyCORS_INT(data=data, numcand=100*data.dim),
                     CandidateUniform(data=data, numcand=100*data.dim)]
weights = [0, 1, 2, 3]
search_strategy = MultiSearchStrategy(search_strategies, weights)
\end{python}

\subsection{Surrogate optimizer} 
We provide a synchronous parallel framework for solving the optimization problem. The method of interest is called optimize and is available in \textit{surrogate\_optimizer.py}. It takes the following input arguments:
\begin{itemize}
\item \textbf{nthreads:} Number of threads (threads) to be used
\item \textbf{maxeval:} Maximal number of function evaluations
\item \textbf{nsample:} Maximal number of simultaneous function evaluations allowed. It is recommended to set this value to nthreads
\item \textbf{response\_surface:} A surrogate model (response surface)
\item \textbf{experimental\_design:} An experimental design to generate initial points from
\item \textbf{search\_strategies:} A strategy for generating new points to evaluate (can be MultiSearchStrategy)
\item \textbf{constraint\_handler:} Method for handling non-bound constraints
\end{itemize}
\ \newline Example: 
\begin{python}
from pySOT import optimize
xbest, fbest = optimize(nthreads=nthreads, data=data, maxeval=maxeval, 
			nsample=nsample, response_surface=rsurface,
                        experimental_design=expdes, search_strategies=sstrat, 
                        constraint_handler=chandle)
\end{python}
The method returns to values, xbest and fbest, which is the best solution found and its objective function value. All of the information from the run is currently printed to an external logfile, ./surrogate\_optimization.log, but if the user want this information available in a structure I can make this possible as well.

\section{An example of how to minimize the 30-dimensional Ackley function}
\begin{python}
import numpy as np

class Ackley:
    #  Global optimum: f(0,0,...,0)=0
    def __init__(self, dim=10):
        self.xlow = -15 * np.ones(dim)
        self.xup = 20 * np.ones(dim)
        self.dim = dim
        self.info = str(dim)+"-dimensional Ackley function \n" +\
                             "Global optimum: f(0,0,...,0) = 0"
        self.min = 0
        self.integer = []
        self.continuous = np.arange(0, dim)
        self.constraints = False
        validate(self)

    def objfunction(self, x):
        if len(x) != self.dim:
            raise ValueError('Dimension mismatch')
        n = float(len(x))
        return -20.0*exp(-0.2*sqrt(sum(x**2)/n)) - \
            exp(sum(cos(2.0*pi*x))/n) + 20 + exp(1)
\end{python}

\begin{python}
from pySOT import CandidateDyCORS,  LatinHypercube, PenaltyMethod, RSCapped, \
                  RBFInterpolant, phi_cubic, linear_tail, dphi_cubic, \
	          dlinear_tail, Ackley, optimize
import numpy as np

if __name__ == "__main__":
    nthreads = 4  # Number of threads
    maxeval = 1000  # Maximal number of function evaluations
    nsample = nthreads  # Maximal number of simultaneous evaluations
    data = Ackley(dim=30)  # Optimization problem
    ed = LatinHypercube(dim=data.dim, npts=2*data.dim+1)  # Experimental design
    rs = RSCapped(RBFInterpolant(phi=phi_cubic, P=linear_tail,
                                 dphi=dphi_cubic, dP=dlinear_tail,
                                 eta=1e-8, maxp=maxeval))  # Surrogate
    ss = CandidateDyCORS(data=data, numcand=100*data.dim)  # Search strategy
    ch = PenaltyMethod(penalty=1.0)  # Constraint handling

    print(data.info)
    # Optimize the objective function in synchronous parallel using nthreads
    # threads and at most maxeval function evaluations
    xbest, fbest = optimize(nthreads=nthreads, data=data, maxeval=maxeval,
                            nsample=nsample, response_surface=rs,
                            experimental_design=ed, search_strategies=ss,
                            constraint_handler=ch)
\end{python}

\section{Calling an external objective function}
Say that we want to evaluate an black-box objective function at a point \\ $x=(x_1,x_2,\ldots,x_n) \in \Rb^n$. We assume that the black-box objective function is an executable file that takes its input of the form \textquotesingle $x_1, \,x_2, \, \ldots, \, x_n$\textquotesingle, i.e., the point where we evaluate is given as a comma separated string of variables. We also assume that the black box objective function returns the value to us by printing it to the terminal, so that we can parse the value of the objective function from the output. We modify the Ackley objective function in our previous example to a script that we refer to AckleyExectuable that has the following form:
\begin{python}
import numpy as np
import sys

x = np.fromstring(sys.argv[1], sep=',')
n = float(len(x))
print(str(-20.0 * np.exp(-0.2*np.sqrt(sum(x**2)/n)) - \
np.exp(sum(np.cos(2.0*np.pi*x))/n) + 20 + np.exp(1)))
\end{python}
This script can be compiled to an executable and will serve as our black-box objective function. The Ackley class can then take the following form:
\begin{python}
import numpy as np
from subprocess32 import Popen, PIPE

class Ackley:
    def __init__(self, dim=10):
        self.xlow = -15 * np.ones(dim)
        self.xup = 20 * np.ones(dim)
        self.dim = dim
        self.info = str(dim)+"-dimensional Ackley function \n" +\
                             "Global optimum: f(0,0,...,0) = 0"
        self.min = 0
        self.integer = []
        self.continuous = np.arange(0, dim)
        self.constraints = False

    def objfunction(self, x):
        if len(x) != self.dim:
            raise ValueError('Dimension mismatch')

        process = Popen(['/PATH/AckleyExecutable', array2str(x)], 
        		stdout=PIPE, stderr=PIPE)
        stdout, stderr = process.communicate()
        return float(stdout)
\end{python}
where /PATH/ is the path of the executable and array2str is a help function that converts a numpy array to the form we described earlier:
\begin{python}
def array2str(x):
    return ",".join(np.char.mod('%f', x))
\end{python}
\textbf{Note:} If your black-box objective function is only being optimized over a subset of the variables or reads its input from an external file the example above will have to be modified appropriately for such a case.

\section{Future changes}
\begin{itemize}
\item Add an asynchronous parallel optimizer
\item Add Heuristic Algorithms to search on the surrogate
\item Add more experimental designs
\item Add more methods for handling constraints
\item Add ensemble surrogates
\item A Graphical User Interface (GUI)
\end{itemize}
\end{document}
